use anyhow::{Context, Result};
use base64::{engine::general_purpose, Engine as _};
use image::{DynamicImage, GenericImageView};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::{Arc, Mutex};

use super::api_client::{ApiClient, Detection};
use crate::models::AnnotationData;

#[cfg(feature = "onnx")]
use lazy_static::lazy_static;

// ONNX æ¨ç†å¼•æ“ç¼“å­˜
#[cfg(feature = "onnx")]
lazy_static! {
    static ref ONNX_ENGINE_CACHE: Mutex<HashMap<String, Arc<super::onnx_inference::OnnxInferenceEngine>>> =
        Mutex::new(HashMap::new());
}

/// æ¨ç†æ¨¡å¼
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type", rename_all = "snake_case")]
pub enum InferenceMode {
    /// ä½¿ç”¨APIæ¨ç†
    Api {
        base_url: String,
        conf_threshold: f32,
        iou_threshold: f32,
    },
    /// ä½¿ç”¨ONNXæ¨ç† (éœ€è¦feature)
    #[cfg(feature = "onnx")]
    Onnx {
        model_path: String,
        conf_threshold: f32,
        iou_threshold: f32,
        use_gpu: bool,
    },
}

/// æ¨ç†é…ç½®
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct InferenceConfig {
    pub mode: InferenceMode,
    pub count: InferenceCount,
}

/// æ¨ç†æ•°é‡é…ç½®
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type", rename_all = "snake_case")]
pub enum InferenceCount {
    /// æ¨ç†æŒ‡å®šæ•°é‡
    Count { value: usize },
    /// æ¨ç†å…¨éƒ¨
    All,
}

/// æ¨ç†ç»“æœ
#[derive(Debug, Serialize)]
pub struct InferenceResult {
    pub image_path: String,
    pub annotations: Vec<AnnotationData>,
    pub inference_time_ms: f32,
}

/// æ‰¹é‡æ¨ç†ç»“æœ
#[derive(Debug, Serialize)]
pub struct BatchInferenceResult {
    pub results: Vec<InferenceResult>,
    pub total_time_ms: f32,
    pub success_count: usize,
    pub error_count: usize,
}

/// æ¨¡å‹æ¨ç†ç®¡ç†å™¨
pub struct InferenceManager {
    config: InferenceConfig,
}

impl InferenceManager {
    pub fn new(config: InferenceConfig) -> Self {
        Self { config }
    }

    /// æ¨ç†å•å¼ å›¾ç‰‡
    pub async fn inference_single(&self, image_path: &str) -> Result<InferenceResult> {
        // åŠ è½½å›¾ç‰‡
        let img = image::open(image_path).context("æ— æ³•æ‰“å¼€å›¾ç‰‡")?;
        let (width, height) = img.dimensions();

        // æ ¹æ®æ¨¡å¼é€‰æ‹©æ¨ç†æ–¹å¼
        let (detections, inference_time) = match &self.config.mode {
            InferenceMode::Api {
                base_url,
                conf_threshold,
                iou_threshold,
            } => {
                self.inference_with_api(&img, base_url, *conf_threshold, *iou_threshold)
                    .await?
            }
            #[cfg(feature = "onnx")]
            InferenceMode::Onnx {
                model_path,
                conf_threshold,
                iou_threshold,
                use_gpu,
            } => self.inference_with_onnx(
                &img,
                model_path,
                *conf_threshold,
                *iou_threshold,
                *use_gpu,
            )?,
        };

        // è½¬æ¢ä¸ºæ ‡æ³¨æ•°æ®
        let annotations = detections
            .into_iter()
            .map(|det| self.detection_to_annotation(det, width, height))
            .collect::<Vec<_>>();

        // println!(
        //     "æ¨ç†å®Œæˆ: {} - æ£€æµ‹åˆ° {} ä¸ªç›®æ ‡",
        //     image_path,
        //     annotations.len()
        // );

        // ä¿å­˜æ ‡æ³¨åˆ°æ–‡ä»¶
        crate::core::annotation::save_annotations_internal(image_path, annotations.clone())
            .map_err(|e| anyhow::anyhow!("æ— æ³•ä¿å­˜æ ‡æ³¨: {}", e.message))?;

        // println!("æ ‡æ³¨å·²ä¿å­˜: {}", image_path);

        Ok(InferenceResult {
            image_path: image_path.to_string(),
            annotations,
            inference_time_ms: inference_time,
        })
    }

    /// æ‰¹é‡æ¨ç†
    pub async fn inference_batch(
        &self,
        image_paths: Vec<String>,
        start_index: usize,
        count: usize,
    ) -> Result<BatchInferenceResult> {
        let start_time = std::time::Instant::now();
        let mut results = Vec::new();
        let mut success_count = 0;
        let mut error_count = 0;

        // è®¡ç®—è¦æ¨ç†çš„å›¾ç‰‡èŒƒå›´
        let end_index = (start_index + count).min(image_paths.len());
        let paths_to_infer = &image_paths[start_index..end_index];

        for path in paths_to_infer {
            match self.inference_single(path).await {
                Ok(result) => {
                    results.push(result);
                    success_count += 1;
                }
                Err(e) => {
                    eprintln!("æ¨ç†å¤±è´¥ {}: {}", path, e);
                    error_count += 1;
                }
            }
        }

        let total_time = start_time.elapsed().as_millis() as f32;

        Ok(BatchInferenceResult {
            results,
            total_time_ms: total_time,
            success_count,
            error_count,
        })
    }

    /// ä½¿ç”¨APIæ¨ç†
    async fn inference_with_api(
        &self,
        img: &DynamicImage,
        base_url: &str,
        conf_threshold: f32,
        iou_threshold: f32,
    ) -> Result<(Vec<Detection>, f32)> {
        // è½¬æ¢å›¾ç‰‡ä¸ºbase64
        let image_base64 = self.image_to_base64(img)?;

        // åˆ›å»ºAPIå®¢æˆ·ç«¯å¹¶æ¨ç†
        let client = ApiClient::new(base_url.to_string());
        let response = client
            .predict(image_base64, conf_threshold, iou_threshold)
            .await?;

        Ok((response.detections, response.inference_time_ms))
    }

    /// ä½¿ç”¨ONNXæ¨ç† (éœ€è¦feature)
    #[cfg(feature = "onnx")]
    fn inference_with_onnx(
        &self,
        img: &DynamicImage,
        model_path: &str,
        conf_threshold: f32,
        iou_threshold: f32,
        use_gpu: bool,
    ) -> Result<(Vec<Detection>, f32)> {
        use super::onnx_inference::OnnxInferenceEngine;

        // ç”Ÿæˆç¼“å­˜keyï¼šåŒ…å«æ¨¡å‹è·¯å¾„å’ŒGPUé…ç½®
        let cache_key = format!("{}:{}:{}", model_path, use_gpu, conf_threshold);

        // å°è¯•ä»ç¼“å­˜è·å–å¼•æ“
        let engine = {
            let mut cache = ONNX_ENGINE_CACHE.lock().unwrap();

            if let Some(cached_engine) = cache.get(&cache_key) {
                // println!("â™»ï¸  å¤ç”¨å·²åŠ è½½çš„ONNXæ¨ç†å¼•æ“");
                Arc::clone(cached_engine)
            } else {
                // println!("ğŸ†• åˆ›å»ºæ–°çš„ONNXæ¨ç†å¼•æ“å¹¶ç¼“å­˜");
                let new_engine = Arc::new(
                    OnnxInferenceEngine::new(model_path, conf_threshold, iou_threshold, use_gpu)
                        .context("æ— æ³•åˆ›å»ºONNXæ¨ç†å™¨")?,
                );
                cache.insert(cache_key.clone(), Arc::clone(&new_engine));
                new_engine
            }
        };

        // æ¨ç†ï¼ˆåªè®¡æ—¶å®é™…æ¨ç†éƒ¨åˆ†ï¼‰
        let start_time = std::time::Instant::now();
        let detections = engine.inference(img).context("ONNXæ¨ç†å¤±è´¥")?;
        let inference_time = start_time.elapsed().as_secs_f32() * 1000.0;

        Ok((detections, inference_time))
    }

    /// å°†å›¾ç‰‡è½¬æ¢ä¸ºbase64
    fn image_to_base64(&self, img: &DynamicImage) -> Result<String> {
        let mut buffer = Vec::new();
        img.write_to(
            &mut std::io::Cursor::new(&mut buffer),
            image::ImageOutputFormat::Jpeg(90),
        )
        .context("æ— æ³•ç¼–ç å›¾ç‰‡")?;

        Ok(general_purpose::STANDARD.encode(&buffer))
    }

    /// å°†æ£€æµ‹ç»“æœè½¬æ¢ä¸ºæ ‡æ³¨æ•°æ®
    fn detection_to_annotation(
        &self,
        detection: Detection,
        img_width: u32,
        img_height: u32,
    ) -> AnnotationData {
        if let Some(angle_deg) = detection.angle {
            // æ—‹è½¬æ¡†ï¼šbboxæ ¼å¼ä¸º[cx, cy, w, h]
            let cx = detection.bbox[0].max(0.0).min(img_width as f32);
            let cy = detection.bbox[1].max(0.0).min(img_height as f32);
            let width = detection.bbox[2].max(1.0);
            let height = detection.bbox[3].max(1.0);

            // å‰ç«¯æœŸæœ›ï¼š
            // 1. x, y æ˜¯å·¦ä¸Šè§’åæ ‡ï¼ˆä¸æ˜¯ä¸­å¿ƒç‚¹ï¼‰
            // 2. rotation æ˜¯å¼§åº¦å€¼ï¼ˆä¸æ˜¯åº¦ï¼‰
            let x_left_top = cx - width / 2.0;
            let y_left_top = cy - height / 2.0;
            let angle_rad = angle_deg * std::f32::consts::PI / 180.0;

            // åˆ›å»ºæ—‹è½¬çŸ©å½¢æ ‡æ³¨
            AnnotationData {
                id: uuid::Uuid::new_v4().to_string(),
                annotation_type: "rotated-rectangle".to_string(),
                x: x_left_top as f64,
                y: y_left_top as f64,
                width: width as f64,
                height: height as f64,
                rotation: Some(angle_rad as f64),
                label: detection.class_name,
                created: chrono::Utc::now().to_rfc3339(),
                visible: true,
            }
        } else {
            // æ™®é€šæ¡†ï¼šbboxæ ¼å¼ä¸º[x_min, y_min, x_max, y_max]
            let x_min = detection.bbox[0].max(0.0).min(img_width as f32);
            let y_min = detection.bbox[1].max(0.0).min(img_height as f32);
            let x_max = detection.bbox[2].max(0.0).min(img_width as f32);
            let y_max = detection.bbox[3].max(0.0).min(img_height as f32);

            let width = (x_max - x_min).max(1.0);
            let height = (y_max - y_min).max(1.0);

            // åˆ›å»ºæ™®é€šçŸ©å½¢æ ‡æ³¨
            // æ³¨æ„ï¼šæ ‡æ³¨ç³»ç»Ÿä¸­ï¼Œæ™®é€šçŸ©å½¢çš„x,yæ˜¯å·¦ä¸Šè§’åæ ‡
            AnnotationData {
                id: uuid::Uuid::new_v4().to_string(),
                annotation_type: "rectangle".to_string(),
                x: x_min as f64,
                y: y_min as f64,
                width: width as f64,
                height: height as f64,
                rotation: None,
                label: detection.class_name,
                created: chrono::Utc::now().to_rfc3339(),
                visible: true,
            }
        }
    }
}
